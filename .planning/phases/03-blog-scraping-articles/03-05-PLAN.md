---
phase: 03-blog-scraping-articles
plan: 05
type: execute
wave: 1
depends_on: []
files_modified:
  - vite.config.ts
  - src/lib/server/scrape-blog.ts
  - src/lib/api/articles.ts
  - src/types/articles.ts
  - package.json
autonomous: true
gap_closure: true

must_haves:
  truths:
    - "SPA mode is disabled — app runs with a Node.js server runtime that executes server functions"
    - "Dev server starts and all existing pages render correctly after disabling SPA mode"
    - "Scrape Blog button fetches articles from the user's blog and populates the table"
    - "Add Article dialog submits a URL and the article appears in the table"
    - "No CORS errors when scraping — all HTTP fetching happens server-side via createServerFn"
    - "RLS is enforced — articles are scoped to the authenticated user's tenant"
  artifacts:
    - path: "vite.config.ts"
      provides: "TanStack Start config with SPA mode disabled"
      contains: "tanstackStart()"
    - path: "src/lib/server/scrape-blog.ts"
      provides: "TanStack Start server functions for blog scraping and single article add"
      exports: ["scrapeBlogFn", "addArticleFn"]
    - path: "src/lib/api/articles.ts"
      provides: "API layer calling server functions instead of Edge Function"
      contains: "import.*scrape-blog"
    - path: "src/types/articles.ts"
      provides: "Updated ScrapeResponse type with 'single' method"
  key_links:
    - from: "vite.config.ts"
      to: "src/server.ts"
      via: "TanStack Start plugin enables server runtime that runs server.ts at startup"
      pattern: "tanstackStart\\(\\)"
    - from: "src/lib/api/articles.ts"
      to: "src/lib/server/scrape-blog.ts"
      via: "direct import and call of server functions (TanStack Start handles client->server RPC)"
      pattern: "scrapeBlogFn|addArticleFn"
    - from: "src/lib/server/scrape-blog.ts"
      to: "supabase"
      via: "server-side Supabase client with user JWT passed from client"
      pattern: "createClient.*Authorization"
---

<objective>
Disable SPA mode to enable the TanStack Start server runtime, then replace Supabase Edge Function invocations with TanStack Start server functions to fix CORS errors blocking blog scraping and manual article addition.

Purpose: The app was migrated to TanStack Start specifically to unlock server capabilities, but SPA mode was left enabled as a transitional step. With `spa: { enabled: true }`, there is NO Node.js server at runtime — `createServerFn` cannot execute. Disabling SPA mode activates the server runtime that already exists at `src/server.ts`, making server functions work. This closes the two blocker gaps from UAT where both "Scrape Blog" and "Add Article" failed with CORS errors.

Output: A working Node.js server runtime with server-side scraping functions that the existing UI components call transparently through the unchanged hooks layer.
</objective>

<execution_context>
@/Users/cweissteiner/.claude/get-shit-done/workflows/execute-plan.md
@/Users/cweissteiner/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-blog-scraping-articles/03-01-SUMMARY.md
@.planning/phases/03-blog-scraping-articles/03-02-SUMMARY.md
@supabase/functions/scrape-blog/index.ts
@src/lib/api/articles.ts
@src/lib/hooks/use-articles.ts
@src/types/articles.ts
@src/lib/supabase.ts
@src/lib/auth.ts
@vite.config.ts
@src/server.ts
@src/client.tsx
@src/router.tsx
</context>

<tasks>

<task type="auto">
  <name>Task 1: Disable SPA mode and verify app works</name>
  <files>vite.config.ts</files>
  <action>
    **This is the prerequisite that makes server functions architecturally possible.**

    In `vite.config.ts`, remove the `spa: { enabled: true }` config from the `tanstackStart()` plugin call. Change:

    ```typescript
    tanstackStart({
      spa: { enabled: true },
    }),
    ```

    To:

    ```typescript
    tanstackStart(),
    ```

    **Why this works:**
    - `src/server.ts` already exists with the correct TanStack Start handler (createStartHandler + defaultStreamHandler + createServerEntry)
    - `src/client.tsx` already hydrates with `StartClient` and `hydrateRoot`
    - `src/router.tsx` already exports `getRouter()` factory
    - `src/routes/__root.tsx` is already the full HTML document shell
    - All existing client-side code (routes, hooks, components, auth) continues to work unchanged
    - The only difference: the build output becomes a Node.js server at `.output/server/index.mjs` instead of static files, and `createServerFn` handlers now execute on that server

    **What changes at runtime:**
    - Dev: `npm run dev` still works the same (Vite dev server now proxies to the Start server handler)
    - Build: `npm run build` outputs to `.output/` (already in `.gitignore`)
    - Production: `npm run start` runs `node .output/server/index.mjs` (script already exists in package.json)

    **Verification after this change:**
    - Run `npm run dev` and confirm the app loads at http://localhost:3000
    - Confirm login/auth still works (client-side Supabase auth is unaffected)
    - Confirm dashboard and project pages render
    - Run `npm run build` and confirm it completes without errors
  </action>
  <verify>
    - `npm run build` passes with zero errors
    - `vite.config.ts` no longer contains `spa:` or `enabled: true`
    - The `tanstackStart()` plugin is called with no arguments (or empty object)
    - `npm run dev` starts without errors (verify by checking the process starts and the dev server responds)
  </verify>
  <done>
    SPA mode is disabled. TanStack Start now runs with a Node.js server runtime, enabling `createServerFn` to execute server-side. All existing client-side functionality is preserved.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create server-side scraping functions and update API layer</name>
  <files>src/lib/server/scrape-blog.ts, src/types/articles.ts, src/lib/api/articles.ts, package.json</files>
  <action>
    **1. Install dependencies:**
    ```bash
    npm install node-html-parser fast-xml-parser
    ```
    - `node-html-parser` for HTML DOM parsing (replaces Deno's built-in DOMParser)
    - `fast-xml-parser` for RSS/Atom XML parsing

    **2. Update `src/types/articles.ts`:**
    - Change ScrapeResponse.method type from `'rss' | 'html'` to `'rss' | 'html' | 'single'` to match what the scraping logic actually returns

    **3. Create `src/lib/server/scrape-blog.ts`:**

    Port the scraping logic from `supabase/functions/scrape-blog/index.ts` (Deno runtime) to TanStack Start server functions (Node.js runtime).

    Create two exported server functions using `createServerFn` from `@tanstack/react-start`:

    **`scrapeBlogFn`** -- Full blog scrape (POST method):
    - Input: `{ blog_project_id: string, blog_url: string, rss_url?: string | null, access_token: string }`
    - Uses `createServerFn({ method: 'POST' })` with `.inputValidator()` for input validation (use a plain validator function, not Zod -- keeps it simple):
      ```typescript
      export const scrapeBlogFn = createServerFn({ method: 'POST' })
        .inputValidator((data: { blog_project_id: string; blog_url: string; rss_url?: string | null; access_token: string }) => {
          if (!data.blog_project_id || !data.blog_url || !data.access_token) {
            throw new Error('blog_project_id, blog_url, and access_token are required')
          }
          return data
        })
        .handler(async ({ data }) => {
          // ... server-side logic here
        })
      ```
    - Creates a Supabase client server-side using `process.env.VITE_SUPABASE_URL` and `process.env.VITE_SUPABASE_ANON_KEY` with the user's `access_token` in the `Authorization: Bearer` header (same pattern as the Edge Function)
    - Gets the user's `tenant_id` by querying the profiles table (exactly as Edge Function does)
    - Tries RSS scraping first (auto-discover if no rss_url provided), then falls back to HTML scraping
    - Upserts articles into `blog_articles` table with `onConflict: 'blog_project_id,url'`
    - Returns `ScrapeResponse` with counts

    **`addArticleFn`** -- Single URL scrape (POST method):
    - Input: `{ blog_project_id: string, single_url: string, access_token: string }`
    - Same `createServerFn({ method: 'POST' }).inputValidator().handler()` pattern
    - Same Supabase client pattern with user JWT
    - Scrapes the single URL for title, content, published_at
    - Upserts into `blog_articles` table
    - Returns `ScrapeResponse` with method: 'single'

    **Authentication approach (addresses BLOCKER 2):**
    The client passes the user's `access_token` as a server function input parameter. This is the correct pattern because:
    - Client gets the token via `supabase.auth.getSession()` (already works)
    - Server function receives it as `data.access_token`
    - Server creates a Supabase client with `Authorization: Bearer ${data.access_token}`
    - RLS is enforced because Supabase sees the user's JWT
    - This is the same auth flow the Edge Function used, just with the token passed as a function parameter instead of an HTTP header

    **Internal helper functions** (not exported, within the same file):
    - `scrapeSingleUrl(url: string)` -- Fetches URL, parses HTML with `node-html-parser`, extracts title (h1 > title tag), content (article > main > body), published_at (time tag > meta tags). Returns `{ title, url, content, published_at }`.
    - `scrapeRss(blog_url: string, rss_url?: string | null)` -- Discovers RSS feed URL if not provided, fetches and parses RSS/Atom XML with `fast-xml-parser`. Handles both RSS 2.0 (`<item>`) and Atom (`<entry>`) formats. Returns array of articles.
    - `discoverRssFeed(blog_url: string)` -- Fetches blog homepage, parses HTML to find `<link type="application/rss+xml">` or `<link type="application/atom+xml">`, tries common paths (`/feed`, `/rss`, `/feed.xml`, `/rss.xml`, `/atom.xml`). Returns feed URL or null.
    - `scrapeHtml(blog_url: string)` -- Fetches blog homepage, finds article links in `<article>` elements or main content area, scrapes each (limit 20). Returns array of articles.

    **Key differences from Deno Edge Function:**
    - Use `node-html-parser`'s `parse()` instead of `new DOMParser().parseFromString()` for HTML
    - Use `fast-xml-parser`'s `XMLParser` instead of `DOMParser` for RSS/XML
    - Use `node-html-parser` query methods: `querySelector()`, `querySelectorAll()`, `getAttribute()`, `innerHTML`, `textContent` (API is similar to DOM)
    - Use `process.env` instead of `Deno.env.get()`
    - Use global `fetch` (available in Node 18+)
    - Import `createClient` from `@supabase/supabase-js` (not from esm.sh URL)

    **RSS parsing with fast-xml-parser:**
    ```typescript
    import { XMLParser } from 'fast-xml-parser'
    const parser = new XMLParser({ ignoreAttributes: false, attributeNamePrefix: '@_' })
    const result = parser.parse(xmlText)
    // RSS 2.0: result.rss.channel.item (array or single object)
    // Atom: result.feed.entry (array or single object)
    ```
    For RSS 2.0: title from `item.title`, link from `item.link`, content from `item['content:encoded'] || item.description`, pubDate from `item.pubDate`
    For Atom: title from `entry.title`, link from `entry.link['@_href']` (or `entry.link` if string), content from `entry.content || entry.summary`, published from `entry.published || entry.updated`

    **Important:** Ensure items are always treated as arrays (fast-xml-parser returns a single object when there's only one item). Use: `const items = Array.isArray(raw) ? raw : [raw]`

    **Error handling:** Wrap all external fetch calls in try/catch. Return meaningful error messages in the errors array. Never throw from the server function -- always return a ScrapeResponse with `success: false` and populated errors array for transport-level failures.

    **4. Update `src/lib/api/articles.ts` to call server functions:**

    Replace `supabase.functions.invoke('scrape-blog')` with direct calls to the server functions.

    **Changes to `scrapeBlog` function:**
    - Import `scrapeBlogFn` from `@/lib/server/scrape-blog`
    - Import `supabase` (already imported) to get the user's access token
    - Get the current session: `const { data: { session } } = await supabase.auth.getSession()`
    - If no session, throw an error
    - Call: `const result = await scrapeBlogFn({ data: { blog_project_id: request.blog_project_id, blog_url: request.blog_url, rss_url: request.rss_url, access_token: session.access_token } })`
    - Return `result` (typed as ScrapeResponse)
    - Remove the `supabase.functions.invoke` call

    **Changes to `addArticleManually` function:**
    - Import `addArticleFn` from `@/lib/server/scrape-blog`
    - Get the current session (same pattern)
    - Call: `const result = await addArticleFn({ data: { blog_project_id: projectId, single_url: url, access_token: session.access_token } })`
    - Return `result`
    - Remove the `supabase.functions.invoke` call

    **Keep unchanged:**
    - All other functions (getArticlesByProject, getArticle, archiveArticle, restoreArticle, getArchivedArticles) -- these use direct Supabase client queries and work fine
    - The `ScrapeRequest` import can be kept (still used for the function signature) or removed if function signature changes

    **How server function calls work (no CORS):**
    TanStack Start server functions are called as regular async functions from client code. At build time, the import is transformed: the client gets a stub that makes an RPC call to the server, and the server runs the actual handler. No manual fetch, no URL, no CORS headers needed.
  </action>
  <verify>
    - `npm run build` passes with zero TypeScript errors
    - `src/lib/server/scrape-blog.ts` exports `scrapeBlogFn` and `addArticleFn`
    - Both functions use `createServerFn({ method: 'POST' }).inputValidator().handler()` pattern
    - Both functions accept `access_token` in input and create authenticated Supabase client
    - `node-html-parser` and `fast-xml-parser` are in package.json dependencies
    - `src/types/articles.ts` ScrapeResponse.method includes 'single'
    - `src/lib/api/articles.ts` no longer references `supabase.functions.invoke`
    - `src/lib/api/articles.ts` imports from `@/lib/server/scrape-blog`
    - No unused imports remain
    - The hooks layer (`src/lib/hooks/use-articles.ts`) requires zero changes
    - The UI components require zero changes
  </verify>
  <done>
    Server functions exist at src/lib/server/scrape-blog.ts with full RSS/HTML/single-URL scraping logic ported from the Edge Function to Node.js runtime. The API layer calls them directly, eliminating CORS. Auth tokens flow from client to server function to Supabase for RLS enforcement.
  </done>
</task>

</tasks>

<verification>
After both tasks complete:

1. **SPA mode disabled:** `vite.config.ts` calls `tanstackStart()` with no SPA config -- server runtime is active
2. **Build check:** `npm run build` passes with zero errors, producing `.output/server/index.mjs`
3. **Dev server works:** `npm run dev` starts and serves the app at http://localhost:3000
4. **Runtime test:** Start dev server, navigate to a project with a blog URL, click "Scrape Blog" -- the server function should execute and return results (or a meaningful error if the blog is unreachable). No CORS errors in browser console.
5. **Import chain intact:** UI components -> hooks -> API functions -> server functions (no broken imports)
6. **No CORS:** `supabase.functions.invoke` is no longer used anywhere in the client-side code for scraping
7. **Auth preserved:** Server functions receive user's access_token and create authenticated Supabase client with RLS
8. **Types consistent:** ScrapeResponse type matches what server functions return
9. **Existing functionality unchanged:** All non-scraping article operations (list, detail, archive, restore) continue to work via direct Supabase queries
10. **Existing pages work:** Dashboard, project list, project detail, article detail pages all render correctly in non-SPA mode
</verification>

<success_criteria>
- SPA mode is disabled and the app runs with a Node.js server runtime
- Blog scraping and manual article addition work through server functions without CORS errors
- The application builds successfully and serves correctly in both dev and production modes
- No changes required to hooks or UI components
- User's authenticated session is propagated to server-side Supabase client for RLS enforcement
- All existing pages and functionality continue to work after the SPA-to-server transition
</success_criteria>

<output>
After completion, create `.planning/phases/03-blog-scraping-articles/03-05-SUMMARY.md`
</output>
