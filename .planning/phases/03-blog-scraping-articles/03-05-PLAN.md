---
phase: 03-blog-scraping-articles
plan: 05
type: execute
wave: 1
depends_on: []
files_modified:
  - src/lib/server/scrape-blog.ts
  - src/lib/api/articles.ts
  - src/types/articles.ts
  - package.json
autonomous: true
gap_closure: true

must_haves:
  truths:
    - "Scrape Blog button fetches articles from the user's blog and populates the table"
    - "Add Article dialog submits a URL and the article appears in the table"
    - "No CORS errors when scraping — all HTTP fetching happens server-side"
    - "RLS is enforced — articles are scoped to the authenticated user's tenant"
  artifacts:
    - path: "src/lib/server/scrape-blog.ts"
      provides: "TanStack Start server functions for blog scraping and single article add"
      exports: ["scrapeBlogFn", "addArticleFn"]
    - path: "src/lib/api/articles.ts"
      provides: "API layer calling server functions instead of Edge Function"
      contains: "import.*scrape-blog"
    - path: "src/types/articles.ts"
      provides: "Updated ScrapeResponse type with 'single' method"
  key_links:
    - from: "src/lib/api/articles.ts"
      to: "src/lib/server/scrape-blog.ts"
      via: "direct import and call of server functions"
      pattern: "scrapeBlogFn|addArticleFn"
    - from: "src/lib/server/scrape-blog.ts"
      to: "supabase"
      via: "server-side Supabase client with user JWT"
      pattern: "createClient.*Authorization"
---

<objective>
Replace Supabase Edge Function invocations with TanStack Start server functions to fix CORS errors blocking blog scraping and manual article addition.

Purpose: The SPA client cannot call Supabase Edge Functions due to CORS restrictions. TanStack Start server functions run server-side and are called via built-in RPC, eliminating CORS entirely. This closes the two blocker gaps from UAT where both "Scrape Blog" and "Add Article" failed.

Output: Server-side scraping functions that the existing UI components call transparently through the unchanged hooks layer.
</objective>

<execution_context>
@/Users/cweissteiner/.claude/get-shit-done/workflows/execute-plan.md
@/Users/cweissteiner/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-blog-scraping-articles/03-01-SUMMARY.md
@.planning/phases/03-blog-scraping-articles/03-02-SUMMARY.md
@supabase/functions/scrape-blog/index.ts
@src/lib/api/articles.ts
@src/lib/hooks/use-articles.ts
@src/types/articles.ts
@src/lib/supabase.ts
@src/lib/auth.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create server-side scraping functions</name>
  <files>src/lib/server/scrape-blog.ts, src/types/articles.ts, package.json</files>
  <action>
    **1. Install dependencies:**
    ```bash
    npm install node-html-parser fast-xml-parser
    ```
    - `node-html-parser` for HTML DOM parsing (replaces Deno's built-in DOMParser)
    - `fast-xml-parser` for RSS/Atom XML parsing

    **2. Update `src/types/articles.ts`:**
    - Change ScrapeResponse.method type from `'rss' | 'html'` to `'rss' | 'html' | 'single'` to match what the scraping logic actually returns

    **3. Create `src/lib/server/scrape-blog.ts`:**

    Port the scraping logic from `supabase/functions/scrape-blog/index.ts` (Deno runtime) to TanStack Start server functions (Node.js runtime).

    Create two exported server functions using `createServerFn` from `@tanstack/react-start`:

    **`scrapeBlogFn`** — Full blog scrape (POST method):
    - Input: `{ blog_project_id: string, blog_url: string, rss_url?: string | null, access_token: string }`
    - Uses `createServerFn({ method: 'POST' })` with Zod input validation via `.inputValidator()`
    - Creates a Supabase client server-side using `process.env.VITE_SUPABASE_URL` and `process.env.VITE_SUPABASE_ANON_KEY` with the user's `access_token` in the `Authorization: Bearer` header (same pattern as the Edge Function)
    - Gets the user's `tenant_id` by querying the profiles table (exactly as Edge Function does)
    - Tries RSS scraping first (auto-discover if no rss_url provided), then falls back to HTML scraping
    - Upserts articles into `blog_articles` table with `onConflict: 'blog_project_id,url'`
    - Returns `ScrapeResponse` with counts

    **`addArticleFn`** — Single URL scrape (POST method):
    - Input: `{ blog_project_id: string, single_url: string, access_token: string }`
    - Same Supabase client pattern with user JWT
    - Scrapes the single URL for title, content, published_at
    - Upserts into `blog_articles` table
    - Returns `ScrapeResponse` with method: 'single'

    **Internal helper functions** (not exported, within the same file):
    - `scrapeSingleUrl(url: string)` — Fetches URL, parses HTML with `node-html-parser`, extracts title (h1 > title tag), content (article > main > body), published_at (time tag > meta tags). Returns `{ title, url, content, published_at }`.
    - `scrapeRss(blog_url: string, rss_url?: string | null)` — Discovers RSS feed URL if not provided, fetches and parses RSS/Atom XML with `fast-xml-parser`. Handles both RSS 2.0 (`<item>`) and Atom (`<entry>`) formats. Returns array of articles.
    - `discoverRssFeed(blog_url: string)` — Fetches blog homepage, parses HTML to find `<link type="application/rss+xml">` or `<link type="application/atom+xml">`, tries common paths (`/feed`, `/rss`, `/feed.xml`, `/rss.xml`, `/atom.xml`). Returns feed URL or null.
    - `scrapeHtml(blog_url: string)` — Fetches blog homepage, finds article links in `<article>` elements or main content area, scrapes each (limit 20). Returns array of articles.

    **Key differences from Deno Edge Function:**
    - Use `node-html-parser`'s `parse()` instead of `new DOMParser().parseFromString()` for HTML
    - Use `fast-xml-parser`'s `XMLParser` instead of `DOMParser` for RSS/XML
    - Use `node-html-parser` query methods: `querySelector()`, `querySelectorAll()`, `getAttribute()`, `innerHTML`, `textContent` (API is similar to DOM)
    - Use `process.env` instead of `Deno.env.get()`
    - Use global `fetch` (available in Node 18+)
    - Import `createClient` from `@supabase/supabase-js` (not from esm.sh URL)

    **RSS parsing with fast-xml-parser:**
    ```typescript
    import { XMLParser } from 'fast-xml-parser'
    const parser = new XMLParser({ ignoreAttributes: false, attributeNamePrefix: '@_' })
    const result = parser.parse(xmlText)
    // RSS 2.0: result.rss.channel.item (array or single object)
    // Atom: result.feed.entry (array or single object)
    ```
    For RSS 2.0: title from `item.title`, link from `item.link`, content from `item['content:encoded'] || item.description`, pubDate from `item.pubDate`
    For Atom: title from `entry.title`, link from `entry.link['@_href']` (or `entry.link` if string), content from `entry.content || entry.summary`, published from `entry.published || entry.updated`

    **Important:** Ensure items are always treated as arrays (fast-xml-parser returns a single object when there's only one item). Use: `const items = Array.isArray(raw) ? raw : [raw]`

    **Error handling:** Wrap all external fetch calls in try/catch. Return meaningful error messages in the errors array. Never throw from the server function — always return a ScrapeResponse with `success: false` and populated errors array for transport-level failures.
  </action>
  <verify>
    - `npm run build` passes with zero TypeScript errors
    - `src/lib/server/scrape-blog.ts` exports `scrapeBlogFn` and `addArticleFn`
    - Both functions use `createServerFn({ method: 'POST' })` pattern
    - Both functions accept `access_token` in input and create authenticated Supabase client
    - `node-html-parser` and `fast-xml-parser` are in package.json dependencies
    - `src/types/articles.ts` ScrapeResponse.method includes 'single'
  </verify>
  <done>
    Server functions exist at src/lib/server/scrape-blog.ts with full RSS/HTML/single-URL scraping logic ported from the Edge Function to Node.js runtime, using node-html-parser and fast-xml-parser for parsing.
  </done>
</task>

<task type="auto">
  <name>Task 2: Update API layer to call server functions</name>
  <files>src/lib/api/articles.ts</files>
  <action>
    Update `src/lib/api/articles.ts` to replace `supabase.functions.invoke('scrape-blog')` with direct calls to the server functions.

    **Changes to `scrapeBlog` function:**
    - Import `scrapeBlogFn` from `@/lib/server/scrape-blog`
    - Import `supabase` (already imported) to get the user's access token
    - Get the current session: `const { data: { session } } = await supabase.auth.getSession()`
    - If no session, throw an error
    - Call `scrapeBlogFn({ data: { blog_project_id, blog_url, rss_url, access_token: session.access_token } })`
    - Return the response (already typed as ScrapeResponse)
    - Remove the `supabase.functions.invoke` call and the wrapping error handling (server function handles errors internally)

    **Changes to `addArticleManually` function:**
    - Import `addArticleFn` from `@/lib/server/scrape-blog`
    - Get the current session (same pattern)
    - Call `addArticleFn({ data: { blog_project_id: projectId, single_url: url, access_token: session.access_token } })`
    - Return the response
    - Remove the `supabase.functions.invoke` call

    **Keep unchanged:**
    - All other functions (getArticlesByProject, getArticle, archiveArticle, restoreArticle, getArchivedArticles) — these use direct Supabase client queries and work fine
    - The `ScrapeRequest` import can be removed since we're no longer using it (the server functions have their own input types)

    **Important:** The server functions are called directly as regular async functions from client code. TanStack Start handles the client->server RPC transparently. The call looks like:
    ```typescript
    const result = await scrapeBlogFn({ data: { ... } })
    ```
    No fetch, no URL, no CORS.
  </action>
  <verify>
    - `npm run build` passes with zero TypeScript errors
    - `src/lib/api/articles.ts` no longer references `supabase.functions.invoke`
    - `src/lib/api/articles.ts` imports from `@/lib/server/scrape-blog`
    - No unused imports remain
    - The hooks layer (`src/lib/hooks/use-articles.ts`) requires zero changes (it calls the same API functions)
    - The UI components require zero changes (they call the same hooks)
  </verify>
  <done>
    The API layer calls TanStack Start server functions instead of Supabase Edge Functions, eliminating CORS issues while preserving the same function signatures consumed by hooks and UI components.
  </done>
</task>

</tasks>

<verification>
After both tasks complete:

1. **Build check:** `npm run build` passes with zero errors
2. **Import chain intact:** UI components -> hooks -> API functions -> server functions (no broken imports)
3. **No CORS:** `supabase.functions.invoke` is no longer used anywhere in the client-side code for scraping
4. **Auth preserved:** Server functions receive user's access_token and create authenticated Supabase client with RLS
5. **Types consistent:** ScrapeResponse type matches what server functions return
6. **Existing functionality unchanged:** All non-scraping article operations (list, detail, archive, restore) continue to work via direct Supabase queries
</verification>

<success_criteria>
- Blog scraping and manual article addition work through server functions without CORS errors
- The application builds successfully
- No changes required to hooks or UI components
- User's authenticated session is propagated to server-side Supabase client for RLS enforcement
</success_criteria>

<output>
After completion, create `.planning/phases/03-blog-scraping-articles/03-05-SUMMARY.md`
</output>
