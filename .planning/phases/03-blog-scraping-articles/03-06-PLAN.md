---
phase: 03-blog-scraping-articles
plan: 06
type: execute
wave: 1
depends_on: []
files_modified:
  - server/inngest/client.ts
  - server/inngest/functions/scrape-blog.ts
  - server/inngest/functions/scrape-single.ts
  - server/inngest/index.ts
  - server/index.ts
  - package.json
  - tsconfig.server.json
  - .env.example
autonomous: true
gap_closure: true

user_setup:
  - service: inngest
    why: "Background job processing for blog scraping"
    env_vars:
      - name: INNGEST_EVENT_KEY
        source: "Inngest Dashboard -> Events -> Event Keys (or use 'test' for local dev with Inngest Dev Server)"
      - name: INNGEST_SIGNING_KEY
        source: "Inngest Dashboard -> Events -> Signing Key (optional for local dev)"

must_haves:
  truths:
    - "Inngest scrape-blog function scrapes RSS feed (with auto-discovery) and HTML fallback, upserts articles to Supabase"
    - "Inngest scrape-single function scrapes a single URL and upserts article to Supabase"
    - "Express server exposes POST /api/scrape endpoint that triggers scraping and returns results"
    - "Express server exposes POST /api/scrape/single endpoint for manual article addition"
    - "Inngest serve endpoint mounted at /api/inngest on Express server"
  artifacts:
    - path: "server/inngest/client.ts"
      provides: "Inngest client instance"
    - path: "server/inngest/functions/scrape-blog.ts"
      provides: "Full blog scrape Inngest function (RSS + HTML fallback)"
      min_lines: 80
    - path: "server/inngest/functions/scrape-single.ts"
      provides: "Single URL scrape Inngest function"
      min_lines: 30
    - path: "server/inngest/index.ts"
      provides: "Functions array export"
    - path: "server/index.ts"
      provides: "Express server with Inngest serve and scraping REST endpoints"
      min_lines: 40
    - path: "tsconfig.server.json"
      provides: "TypeScript config for server directory (Node.js target)"
  key_links:
    - from: "server/index.ts"
      to: "server/inngest/client.ts"
      via: "import inngest client + functions for serve()"
      pattern: "serve.*inngest"
    - from: "server/inngest/functions/scrape-blog.ts"
      to: "supabase"
      via: "Supabase client with service role key for writes"
      pattern: "createClient.*SUPABASE"
---

<objective>
Set up an Inngest-powered Express server for blog scraping that replaces the broken Supabase Edge Function.

Purpose: The existing Edge Function hits CORS errors when called from the SPA. TanStack Start server functions are incompatible with Vite 7. An Express server with Inngest provides a clean, working scraping backend with durable execution and retries.

Output: A standalone Express server at `server/` with Inngest functions for blog scraping (RSS + HTML + single URL), REST endpoints for the client to call, and the Inngest serve endpoint for function management.
</objective>

<execution_context>
@/Users/cweissteiner/.claude/get-shit-done/workflows/execute-plan.md
@/Users/cweissteiner/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-blog-scraping-articles/03-01-SUMMARY.md
@.planning/phases/03-blog-scraping-articles/03-05-SUMMARY.md

# Source scraping logic to port from Deno to Node.js:
@supabase/functions/scrape-blog/index.ts

# Current client API layer (will be updated in Plan 07):
@src/lib/api/articles.ts
@src/types/articles.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install dependencies and create server infrastructure</name>
  <files>
    package.json
    tsconfig.server.json
    .env.example
    server/inngest/client.ts
    server/inngest/index.ts
  </files>
  <action>
    **1. Install dependencies:**
    ```bash
    npm install inngest express cors
    npm install -D @types/express @types/cors tsx
    ```
    - `inngest` — Inngest SDK for creating functions and serve endpoint
    - `express` + `cors` — HTTP server (separate from TanStack Start's Vite dev server)
    - `tsx` — Run TypeScript server files directly without pre-compilation
    - `@types/express` + `@types/cors` — TypeScript types

    **2. Create `tsconfig.server.json`:**
    ```json
    {
      "compilerOptions": {
        "target": "ES2022",
        "module": "ESNext",
        "moduleResolution": "bundler",
        "esModuleInterop": true,
        "strict": true,
        "outDir": "./dist/server",
        "rootDir": "./server",
        "skipLibCheck": true,
        "resolveJsonModule": true,
        "declaration": false,
        "types": ["node"]
      },
      "include": ["server/**/*.ts"]
    }
    ```
    This keeps the server TypeScript config separate from the Vite/client config.

    **3. Update `.env.example`** — Add new env vars (append, don't overwrite existing):
    ```
    # Inngest Configuration
    INNGEST_EVENT_KEY=test
    INNGEST_SIGNING_KEY=

    # Supabase Service Role (for server-side writes bypassing RLS)
    SUPABASE_URL=your-project-url
    SUPABASE_SECRET_KEY=your-service-role-key
    ```
    Note: The server uses `SUPABASE_URL` (not `VITE_SUPABASE_URL`) and `SUPABASE_SECRET_KEY` (not anon key) because the server writes articles on behalf of users. The service role bypasses RLS, so the server must enforce tenant isolation in code.

    **4. Create `server/inngest/client.ts`:**
    ```typescript
    import { Inngest } from 'inngest'

    export const inngest = new Inngest({
      id: 'petra-pinterest',
    })
    ```

    **5. Create `server/inngest/index.ts`:**
    ```typescript
    export { inngest } from './client'
    // Functions will be imported after Task 2 creates them
    ```
    This file will be updated in Task 2 to export the functions array.

    **6. Add server scripts to `package.json`** (add to "scripts" object):
    ```json
    "dev:server": "tsx watch server/index.ts",
    "start:server": "tsx server/index.ts"
    ```
    Development workflow: Run `npm run dev` (Vite) and `npm run dev:server` (Express) in separate terminals.
  </action>
  <verify>
    - `npm ls inngest express cors tsx` shows all packages installed
    - `tsconfig.server.json` exists with Node.js-appropriate config
    - `server/inngest/client.ts` exists and exports inngest instance
    - `.env.example` contains INNGEST_EVENT_KEY, SUPABASE_URL, SUPABASE_SECRET_KEY
    - `package.json` has dev:server and start:server scripts
  </verify>
  <done>
    Dependencies installed, server TypeScript config created, Inngest client initialized, and dev scripts ready.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create Inngest scraping functions and Express server</name>
  <files>
    server/inngest/functions/scrape-blog.ts
    server/inngest/functions/scrape-single.ts
    server/inngest/index.ts
    server/index.ts
  </files>
  <action>
    **1. Create `server/inngest/functions/scrape-blog.ts`:**

    Port the scraping logic from `supabase/functions/scrape-blog/index.ts` (Deno) to Node.js. Key changes:
    - Replace Deno's `DOMParser` with `node-html-parser` for HTML parsing
    - Replace XML DOMParser with `fast-xml-parser` for RSS/Atom parsing
    - Use `SUPABASE_URL` + `SUPABASE_SECRET_KEY` env vars (not user JWT — the server is trusted)
    - Accept `tenant_id` as a parameter (passed by the REST endpoint after auth verification)

    Install the parsing libraries:
    ```bash
    npm install node-html-parser fast-xml-parser
    ```

    The function should:
    ```typescript
    import { inngest } from '../client'
    import { createClient } from '@supabase/supabase-js'
    import { parse as parseHTML } from 'node-html-parser'
    import { XMLParser } from 'fast-xml-parser'

    interface ScrapeBlogEvent {
      data: {
        blog_project_id: string
        blog_url: string
        rss_url?: string | null
        tenant_id: string
      }
    }

    interface ArticleData {
      title: string
      url: string
      content?: string
      published_at?: string
    }

    export const scrapeBlog = inngest.createFunction(
      { id: 'scrape-blog' },
      { event: 'blog/scrape.requested' },
      async ({ event, step }) => {
        const { blog_project_id, blog_url, rss_url, tenant_id } = event.data

        // Create Supabase client with service role
        const supabase = createClient(
          process.env.SUPABASE_URL!,
          process.env.SUPABASE_SECRET_KEY!
        )

        // Step 1: Try RSS scraping
        const rssResult = await step.run('scrape-rss', async () => {
          try {
            const articles = await scrapeRss(blog_url, rss_url || undefined)
            return { articles, method: 'rss' as const }
          } catch (error) {
            return { articles: [] as ArticleData[], method: 'rss' as const, error: String(error) }
          }
        })

        // Step 2: HTML fallback if RSS failed
        let articles = rssResult.articles
        let method: 'rss' | 'html' = rssResult.articles.length > 0 ? 'rss' : 'html'
        const errors: string[] = []

        if (rssResult.error) errors.push(`RSS failed: ${rssResult.error}`)

        if (articles.length === 0) {
          const htmlResult = await step.run('scrape-html', async () => {
            try {
              return { articles: await scrapeHtml(blog_url) }
            } catch (error) {
              return { articles: [] as ArticleData[], error: String(error) }
            }
          })
          articles = htmlResult.articles
          if (htmlResult.error) errors.push(`HTML scraping failed: ${htmlResult.error}`)
        }

        if (articles.length === 0) {
          return {
            success: false,
            articles_found: 0,
            articles_created: 0,
            articles_updated: 0,
            method,
            errors: errors.length > 0 ? errors : ['No articles found'],
          }
        }

        // Step 3: Upsert articles to database
        const upsertResult = await step.run('upsert-articles', async () => {
          let created = 0
          let updated = 0
          const upsertErrors: string[] = []

          for (const article of articles) {
            try {
              const { data: existing } = await supabase
                .from('blog_articles')
                .select('id')
                .eq('blog_project_id', blog_project_id)
                .eq('url', article.url)
                .single()

              const { error: upsertError } = await supabase
                .from('blog_articles')
                .upsert({
                  tenant_id,
                  blog_project_id,
                  title: article.title,
                  url: article.url,
                  content: article.content,
                  published_at: article.published_at,
                  scraped_at: new Date().toISOString(),
                }, {
                  onConflict: 'blog_project_id,url',
                })

              if (upsertError) {
                upsertErrors.push(`Failed to upsert ${article.url}: ${upsertError.message}`)
              } else {
                if (existing) updated++
                else created++
              }
            } catch (error) {
              upsertErrors.push(`Error processing ${article.url}: ${String(error)}`)
            }
          }

          return { created, updated, errors: upsertErrors }
        })

        return {
          success: true,
          articles_found: articles.length,
          articles_created: upsertResult.created,
          articles_updated: upsertResult.updated,
          method,
          errors: [...errors, ...upsertResult.errors],
        }
      }
    )
    ```

    Then implement and **export** the helper functions below the Inngest function. Port each function line-by-line from `supabase/functions/scrape-blog/index.ts`, replacing Deno's `DOMParser` with the Node.js equivalents. All four helpers must be exported so `scrape-single.ts` and `server/index.ts` can import them.

    **Export statement** (place at bottom of file):
    ```typescript
    export { scrapeRss, scrapeHtml, scrapeSingleUrl, discoverRssFeed }
    ```

    **`discoverRssFeed(blogUrl: string): Promise<string | null>`**
    - Fetch the blog homepage HTML
    - Parse with `parseHTML(html)` (from `node-html-parser`)
    - Find `<link>` tags: `root.querySelectorAll('link')` then filter by `getAttribute('type')` matching `'application/rss+xml'` or `'application/atom+xml'`
    - If found, return the `href` attribute (resolve relative URLs against `blogUrl` using `new URL(href, blogUrl).toString()`)
    - If not found, try common feed paths in order: `/feed`, `/rss`, `/feed.xml`, `/rss.xml`, `/atom.xml` — for each, do a HEAD request and check for 200 status
    - Return the first working feed URL, or `null` if none found

    **`scrapeRss(blogUrl: string, rssUrl?: string): Promise<ArticleData[]>`**
    - If `rssUrl` not provided, call `discoverRssFeed(blogUrl)` — if null, return empty array
    - Fetch the RSS feed URL
    - Parse XML with `fast-xml-parser`:
      ```typescript
      const parser = new XMLParser({ ignoreAttributes: false, attributeNamePrefix: '@_' })
      const parsed = parser.parse(xmlText)
      ```
    - Handle RSS 2.0 format: items at `parsed.rss?.channel?.item`
    - Handle Atom format: entries at `parsed.feed?.entry`
    - **Important:** Normalize to array — `fast-xml-parser` returns a single object when there's only one item: `const items = Array.isArray(raw) ? raw : raw ? [raw] : []`
    - Map each item to `ArticleData`:
      - RSS 2.0: `title` from `item.title`, `url` from `item.link`, `content` from `item['content:encoded'] || item.description`, `published_at` from `item.pubDate`
      - Atom: `title` from `entry.title`, `url` from `entry.link?.['@_href']` (or if link is an array, find the one with `@_rel === 'alternate'`, or take `entry.link` if it's a string), `content` from `entry.content || entry.summary`, `published_at` from `entry.published || entry.updated`
    - Return array of `ArticleData`

    **`scrapeHtml(blogUrl: string): Promise<ArticleData[]>`**
    - Fetch the blog homepage
    - Parse with `parseHTML(html)`
    - Find article links: look for `<article>` elements, then extract `<a>` href from each; fallback to `root.querySelectorAll('a')` filtered to same-domain links
    - Limit to 20 URLs
    - For each URL, call `scrapeSingleUrl(url)` — wrap in try/catch and skip failures
    - Return array of successfully scraped `ArticleData`

    **`scrapeSingleUrl(url: string): Promise<ArticleData>`**
    - Fetch the URL
    - Parse with `parseHTML(html)`
    - Extract title: `root.querySelector('h1')?.textContent || root.querySelector('title')?.textContent || url`
    - Extract content: `root.querySelector('article')?.innerHTML || root.querySelector('main')?.innerHTML || root.querySelector('body')?.innerHTML || ''`
    - Extract published_at: `root.querySelector('time')?.getAttribute('datetime')` or look for `meta[property="article:published_time"]` getAttribute('content')
    - Return `{ title, url, content, published_at }`

    **API mapping from Deno DOMParser to node-html-parser:**
    | Deno DOMParser | node-html-parser |
    |----------------|-----------------|
    | `new DOMParser().parseFromString(html, 'text/html')` | `parseHTML(html)` (imported as `parse` renamed to `parseHTML`) |
    | `doc.querySelector('h1')` | `root.querySelector('h1')` |
    | `doc.querySelectorAll('a')` | `root.querySelectorAll('a')` |
    | `el.getAttribute('href')` | `el.getAttribute('href')` (same API) |
    | `el.textContent` | `el.textContent` (same API) |
    | `el.innerHTML` | `el.innerHTML` (same API) |

    **Key differences from Edge Function:**
    - `fetch()` is available natively in Node.js 18+ (no polyfill needed)
    - Use `node-html-parser`'s `parse()` aliased as `parseHTML` instead of `new DOMParser().parseFromString()`
    - Use `fast-xml-parser`'s `XMLParser` instead of `DOMParser` for XML/RSS
    - Error handling: use `String(error)` instead of `error.message` to handle non-Error throws

    **2. Create `server/inngest/functions/scrape-single.ts`:**

    ```typescript
    import { inngest } from '../client'
    import { createClient } from '@supabase/supabase-js'
    // Import scrapeSingleUrl from scrape-blog.ts (export it)
    import { scrapeSingleUrl } from './scrape-blog'

    export const scrapeSingle = inngest.createFunction(
      { id: 'scrape-single-article' },
      { event: 'blog/scrape-single.requested' },
      async ({ event, step }) => {
        const { blog_project_id, url, tenant_id } = event.data

        const supabase = createClient(
          process.env.SUPABASE_URL!,
          process.env.SUPABASE_SECRET_KEY!
        )

        const result = await step.run('scrape-and-upsert', async () => {
          const article = await scrapeSingleUrl(url)

          const { error: upsertError } = await supabase
            .from('blog_articles')
            .upsert({
              tenant_id,
              blog_project_id,
              title: article.title,
              url: article.url,
              content: article.content,
              published_at: article.published_at,
              scraped_at: new Date().toISOString(),
            }, {
              onConflict: 'blog_project_id,url',
            })

          if (upsertError) {
            throw new Error(upsertError.message)
          }

          return {
            success: true,
            articles_found: 1,
            articles_created: 1,
            articles_updated: 0,
            method: 'single' as const,
            errors: [],
          }
        })

        return result
      }
    )
    ```

    **3. Update `server/inngest/index.ts`** to export functions array:
    ```typescript
    export { inngest } from './client'
    export { scrapeBlog } from './functions/scrape-blog'
    export { scrapeSingle } from './functions/scrape-single'

    import { scrapeBlog } from './functions/scrape-blog'
    import { scrapeSingle } from './functions/scrape-single'

    export const functions = [scrapeBlog, scrapeSingle]
    ```

    **4. Create `server/index.ts`:**

    The Express server serves two purposes:
    a) Inngest serve endpoint at `/api/inngest` (for Inngest Dev Server / Cloud to invoke functions)
    b) REST endpoints at `/api/scrape` and `/api/scrape/single` for the client to trigger scrapes

    The REST endpoints do NOT go through Inngest's event system for v1 — they call the scraping logic directly for synchronous responses. This keeps the UX simple (client waits for result). Inngest wrapping is available for future background/scheduled scrapes.

    ```typescript
    import express from 'express'
    import cors from 'cors'
    import { serve } from 'inngest/express'
    import { inngest, functions } from './inngest'
    import { createClient } from '@supabase/supabase-js'
    import { scrapeRss, scrapeHtml, scrapeSingleUrl } from './inngest/functions/scrape-blog'

    const app = express()
    const PORT = process.env.PORT || 3001

    app.use(express.json())
    app.use(cors({
      origin: ['http://localhost:3000', process.env.CLIENT_ORIGIN || ''].filter(Boolean),
      credentials: true,
    }))

    // Inngest serve endpoint
    app.use('/api/inngest', serve({ client: inngest, functions }))

    // Helper: verify Supabase auth token and get user info
    async function verifyAuth(authHeader: string | undefined) {
      if (!authHeader || !authHeader.startsWith('Bearer ')) {
        throw new Error('Missing or invalid Authorization header')
      }

      const token = authHeader.replace('Bearer ', '')
      const supabase = createClient(
        process.env.SUPABASE_URL!,
        process.env.SUPABASE_SECRET_KEY!
      )

      const { data: { user }, error } = await supabase.auth.getUser(token)
      if (error || !user) {
        throw new Error('Invalid auth token')
      }

      // Get tenant_id from profile
      const { data: profile } = await supabase
        .from('profiles')
        .select('tenant_id')
        .eq('id', user.id)
        .single()

      if (!profile) {
        throw new Error('Profile not found')
      }

      return { user, tenant_id: profile.tenant_id }
    }

    // POST /api/scrape — Full blog scrape
    app.post('/api/scrape', async (req, res) => {
      try {
        const { user: _, tenant_id } = await verifyAuth(req.headers.authorization)
        const { blog_project_id, blog_url, rss_url } = req.body

        if (!blog_project_id || !blog_url) {
          return res.status(400).json({
            success: false,
            errors: ['blog_project_id and blog_url are required']
          })
        }

        const supabase = createClient(
          process.env.SUPABASE_URL!,
          process.env.SUPABASE_SECRET_KEY!
        )

        // Try RSS first
        let articles: { title: string; url: string; content?: string; published_at?: string }[] = []
        let method: 'rss' | 'html' = 'html'
        const errors: string[] = []

        try {
          articles = await scrapeRss(blog_url, rss_url || undefined)
          if (articles.length > 0) method = 'rss'
        } catch (error) {
          errors.push(`RSS failed: ${String(error)}`)
        }

        if (articles.length === 0) {
          try {
            articles = await scrapeHtml(blog_url)
            method = 'html'
          } catch (error) {
            errors.push(`HTML scraping failed: ${String(error)}`)
          }
        }

        if (articles.length === 0) {
          return res.json({
            success: false,
            articles_found: 0,
            articles_created: 0,
            articles_updated: 0,
            method,
            errors: errors.length > 0 ? errors : ['No articles found'],
          })
        }

        // Upsert articles
        let created = 0
        let updated = 0

        for (const article of articles) {
          try {
            const { data: existing } = await supabase
              .from('blog_articles')
              .select('id')
              .eq('blog_project_id', blog_project_id)
              .eq('url', article.url)
              .single()

            const { error: upsertError } = await supabase
              .from('blog_articles')
              .upsert({
                tenant_id,
                blog_project_id,
                title: article.title,
                url: article.url,
                content: article.content,
                published_at: article.published_at,
                scraped_at: new Date().toISOString(),
              }, { onConflict: 'blog_project_id,url' })

            if (upsertError) {
              errors.push(`Failed to upsert ${article.url}: ${upsertError.message}`)
            } else {
              if (existing) updated++
              else created++
            }
          } catch (error) {
            errors.push(`Error processing ${article.url}: ${String(error)}`)
          }
        }

        res.json({
          success: true,
          articles_found: articles.length,
          articles_created: created,
          articles_updated: updated,
          method,
          errors,
        })
      } catch (error) {
        const message = error instanceof Error ? error.message : String(error)
        res.status(message.includes('auth') || message.includes('token') ? 401 : 500)
          .json({ success: false, errors: [message] })
      }
    })

    // POST /api/scrape/single — Single article scrape
    app.post('/api/scrape/single', async (req, res) => {
      try {
        const { user: _, tenant_id } = await verifyAuth(req.headers.authorization)
        const { blog_project_id, url } = req.body

        if (!blog_project_id || !url) {
          return res.status(400).json({
            success: false,
            errors: ['blog_project_id and url are required']
          })
        }

        const supabase = createClient(
          process.env.SUPABASE_URL!,
          process.env.SUPABASE_SECRET_KEY!
        )

        const article = await scrapeSingleUrl(url)

        const { error: upsertError } = await supabase
          .from('blog_articles')
          .upsert({
            tenant_id,
            blog_project_id,
            title: article.title,
            url: article.url,
            content: article.content,
            published_at: article.published_at,
            scraped_at: new Date().toISOString(),
          }, { onConflict: 'blog_project_id,url' })

        if (upsertError) {
          return res.status(500).json({
            success: false,
            errors: [upsertError.message],
          })
        }

        res.json({
          success: true,
          articles_found: 1,
          articles_created: 1,
          articles_updated: 0,
          method: 'single',
          errors: [],
        })
      } catch (error) {
        const message = error instanceof Error ? error.message : String(error)
        res.status(message.includes('auth') || message.includes('token') ? 401 : 500)
          .json({ success: false, errors: [message] })
      }
    })

    app.listen(PORT, () => {
      console.log(`Scraping server running on http://localhost:${PORT}`)
      console.log(`Inngest endpoint: http://localhost:${PORT}/api/inngest`)
    })
    ```

    **Important implementation notes:**
    - The `scrapeRss`, `scrapeHtml`, and `scrapeSingleUrl` helper functions must be exported from `scrape-blog.ts` so both the Inngest functions AND the Express endpoints can use them
    - Auth verification uses the user's Bearer token (from Supabase session) to look up tenant_id, ensuring multi-tenant isolation
    - The Express endpoints call scraping helpers directly for synchronous responses (v1 simplicity)
    - The Inngest functions wrap the same logic for future background/scheduled scraping
    - The server runs on port 3001 to avoid conflict with Vite dev server on port 3000
  </action>
  <verify>
    - `npx tsx server/index.ts` starts without errors (test briefly, then Ctrl+C)
    - All files exist: server/index.ts, server/inngest/client.ts, server/inngest/index.ts, server/inngest/functions/scrape-blog.ts, server/inngest/functions/scrape-single.ts
    - `npx tsc --noEmit -p tsconfig.server.json` passes with no type errors
    - The Express server imports and mounts Inngest serve correctly
  </verify>
  <done>
    Express server with Inngest integration running on port 3001, scraping functions ported from Edge Function to Node.js, REST endpoints for synchronous scrape triggering, and Inngest serve endpoint for function management.
  </done>
</task>

</tasks>

<verification>
1. `npm run dev:server` starts Express on port 3001 without errors
2. `curl http://localhost:3001/api/inngest` returns Inngest introspection response
3. TypeScript compilation passes: `npx tsc --noEmit -p tsconfig.server.json`
4. All scraping helper functions (scrapeRss, scrapeHtml, scrapeSingleUrl, discoverRssFeed) are implemented and exported
</verification>

<success_criteria>
- Express server starts on port 3001 with Inngest serve endpoint and REST scraping endpoints
- Scraping logic fully ported from Deno Edge Function to Node.js (node-html-parser + fast-xml-parser)
- Auth verification extracts tenant_id from Supabase session token for multi-tenant isolation
- Both full blog scrape and single URL scrape endpoints return ScrapeResponse-shaped JSON
</success_criteria>

<output>
After completion, create `.planning/phases/03-blog-scraping-articles/03-06-SUMMARY.md`
</output>
