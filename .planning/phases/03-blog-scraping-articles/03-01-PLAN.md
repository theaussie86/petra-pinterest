---
phase: 03-blog-scraping-articles
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - supabase/migrations/00004_blog_articles.sql
  - supabase/functions/scrape-blog/index.ts
autonomous: true

must_haves:
  truths:
    - "blog_articles table exists with tenant isolation via RLS"
    - "Articles support soft delete (archived_at column)"
    - "Scrape Edge Function parses RSS feeds and extracts article data"
    - "Scrape Edge Function falls back to HTML scraping when RSS unavailable"
    - "Scrape Edge Function upserts articles (updates existing, inserts new)"
  artifacts:
    - path: "supabase/migrations/00004_blog_articles.sql"
      provides: "blog_articles table with RLS policies"
      contains: "CREATE TABLE public.blog_articles"
    - path: "supabase/functions/scrape-blog/index.ts"
      provides: "Edge Function for blog scraping"
      contains: "Deno.serve"
  key_links:
    - from: "supabase/functions/scrape-blog/index.ts"
      to: "blog_articles table"
      via: "Supabase client insert/upsert"
      pattern: "from.*blog_articles.*upsert"
---

<objective>
Create the blog_articles database table with multi-tenant RLS and a Supabase Edge Function that scrapes blog articles via RSS (with HTML fallback).

Purpose: Establishes the data foundation and scraping backend for Phase 3. Articles table stores scraped content; Edge Function handles the actual blog scraping logic server-side (avoids CORS issues from the SPA).

Output: Database migration applied, Edge Function deployable via `supabase functions deploy scrape-blog`.
</objective>

<execution_context>
@/Users/cweissteiner/.claude/get-shit-done/workflows/execute-plan.md
@/Users/cweissteiner/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-blog-scraping-articles/03-CONTEXT.md
@supabase/migrations/00002_blog_projects.sql
@src/types/blog-projects.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create blog_articles table migration with RLS</name>
  <files>supabase/migrations/00004_blog_articles.sql</files>
  <action>
Create migration file `supabase/migrations/00004_blog_articles.sql` with:

**Table: blog_articles**
```sql
CREATE TABLE public.blog_articles (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  tenant_id UUID NOT NULL,
  blog_project_id UUID NOT NULL REFERENCES public.blog_projects(id) ON DELETE CASCADE,
  title TEXT NOT NULL,
  url TEXT NOT NULL,
  content TEXT,  -- Full scraped article content (HTML)
  published_at TIMESTAMPTZ,  -- Original publish date from blog
  scraped_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),  -- Last scrape timestamp
  archived_at TIMESTAMPTZ,  -- Soft delete: NULL = active, non-NULL = archived
  created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
  updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);
```

**Unique constraint** on (blog_project_id, url) to prevent duplicate articles per project — enables upsert behavior.

**RLS policies** (same pattern as blog_projects):
- SELECT: `tenant_id IN (SELECT tenant_id FROM public.profiles WHERE id = (SELECT auth.uid()))`
- INSERT: Same check + `tenant_id = (SELECT tenant_id FROM public.profiles WHERE id = (SELECT auth.uid()))`
- UPDATE: Same as SELECT
- DELETE: Same as SELECT

**Indexes:**
- `idx_blog_articles_tenant_id` on tenant_id
- `idx_blog_articles_blog_project_id` on blog_project_id
- `idx_blog_articles_published_at` on published_at DESC (for default sort)
- `idx_blog_articles_archived_at` on archived_at (for filtering active articles)

**Trigger:** Auto-update `updated_at` using the same trigger function pattern from blog_projects migration (reuse `update_updated_at_column` if it exists, otherwise create).

Apply migration with `supabase db push`.
  </action>
  <verify>
Run `supabase db push` successfully. Verify table exists by checking migration output. Run `npm run build` to ensure no TypeScript regressions.
  </verify>
  <done>
blog_articles table exists in Supabase with RLS enabled, unique constraint on (blog_project_id, url), performance indexes, soft-delete column, and auto-update trigger.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create scrape-blog Supabase Edge Function</name>
  <files>supabase/functions/scrape-blog/index.ts</files>
  <action>
Create Supabase Edge Function at `supabase/functions/scrape-blog/index.ts` using Deno runtime.

**Function signature:** POST endpoint accepting:
```json
{
  "blog_project_id": "uuid",
  "blog_url": "https://example.com",
  "rss_url": "https://example.com/feed" // optional
}
```

**Authentication:** Extract the user's JWT from the Authorization header. Create a Supabase client using the user's JWT (not the service role key) so RLS applies. Get the user's tenant_id from their profile.

**Scraping logic:**

1. **Try RSS first:**
   - If `rss_url` provided, fetch that URL
   - If not provided, try auto-discovery: fetch `blog_url` and look for `<link rel="alternate" type="application/rss+xml">` or `<link rel="alternate" type="application/atom+xml">` in the HTML head
   - Common RSS paths to try if auto-discovery fails: `/feed`, `/rss`, `/feed.xml`, `/rss.xml`, `/atom.xml`
   - Parse RSS/Atom XML manually (no external library needed for basic XML parsing in Deno — use DOMParser from the web API or simple regex for `<item>` / `<entry>` elements)
   - Extract from each item: title, link/url, pubDate/published, content:encoded or description

2. **HTML fallback (if RSS fails):**
   - Fetch the blog_url
   - Parse HTML to find article links (look for `<article>`, `<a>` elements within main content areas)
   - For each article URL found, fetch the page and extract: title from `<h1>` or `<title>`, content from `<article>` or main content area, date from `<time>` or meta tags
   - Limit to most recent 20 articles to avoid overload

3. **Upsert articles:**
   - For each extracted article, upsert into blog_articles using the unique constraint on (blog_project_id, url)
   - Set tenant_id from the authenticated user's profile
   - On conflict (same project + URL): update title, content, published_at, scraped_at
   - On insert: set all fields

4. **Return response:**
```json
{
  "success": true,
  "articles_found": 15,
  "articles_created": 10,
  "articles_updated": 5,
  "method": "rss" | "html",
  "errors": []  // Any per-article errors
}
```

**Error handling:**
- If RSS and HTML both fail, return error with helpful message
- Individual article fetch failures should not block others (collect errors, continue)
- Return appropriate HTTP status codes (200 for success, 400 for bad input, 500 for server errors)

**CORS headers:** Include `Access-Control-Allow-Origin: *` and handle OPTIONS preflight for the SPA to call this function.

**Important Deno notes:**
- Use `Deno.serve()` pattern for the function handler
- Import createClient from `https://esm.sh/@supabase/supabase-js@2`
- Use standard Fetch API for HTTP requests
- Use DOMParser from Deno's web API for HTML/XML parsing (available in Deno runtime)
  </action>
  <verify>
File exists at `supabase/functions/scrape-blog/index.ts`. Verify syntax by reviewing the file. Deploy with `supabase functions deploy scrape-blog` (may require Supabase CLI auth). If deploy is not possible now, verify the file has correct Deno syntax and exports.
  </verify>
  <done>
Edge Function file exists with RSS parsing, HTML fallback, article upsert logic, CORS headers, and proper error handling. Function is deployable via Supabase CLI.
  </done>
</task>

</tasks>

<verification>
1. Migration `00004_blog_articles.sql` applied successfully
2. `blog_articles` table has RLS enabled with tenant isolation policies
3. Unique constraint on (blog_project_id, url) exists
4. Edge Function file has valid Deno/TypeScript syntax
5. Edge Function handles both RSS and HTML scraping paths
6. `npm run build` passes (no TypeScript regressions)
</verification>

<success_criteria>
- blog_articles table exists in Supabase with RLS, soft-delete, and performance indexes
- scrape-blog Edge Function is ready for deployment
- Both RSS and HTML fallback code paths exist in the function
- Articles are upserted (not duplicated) based on project + URL
</success_criteria>

<output>
After completion, create `.planning/phases/03-blog-scraping-articles/03-01-SUMMARY.md`
</output>
